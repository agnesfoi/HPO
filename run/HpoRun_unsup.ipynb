{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from itertools import chain\n",
    "from time import strftime, localtime\n",
    "os.chdir('C://Users/Jingz/OneDrive/CarrierGene/2019.07.10-HPO (done)/NLP_for_onedrive')\n",
    "from NLP_function import *\n",
    "os.chdir('C://Users/Jingz/OneDrive/CarrierGene/2019.07.10-HPO (done)/NLP_for_onedrive/Run')\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * parameters *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "file = 'C:/Users/Jingz/OneDrive/CarrierGene/2019.07.10-HPO (done)/NLP_for_onedrive/Data_Prepare/Project_pogress_20180509.xlsx'\n",
    "#sampid = ['A2172','A4305','A2769','A2023', 'A2042','A2108',\n",
    "#          'A3455','A3450','A3439','A3435','A3444','A4304','A3691','A32940','A2023']\n",
    "sampid = ['A4537','A4222','A4382','A4442','A3581','A4583']\n",
    "#sampid = ['A3324']\n",
    "topn = 10 # choose top n terms to show (doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data import _ use original clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use original clinical data\n",
    "df = pd.read_excel(file)\n",
    "df = df.iloc[:, np.r_[0, 5, 17, 21:58]]\n",
    "df = df.loc[df['样本编号'].isin(sampid)]\n",
    "\n",
    "# merge title and content\n",
    "for a in range(0,df.shape[1]-1):\n",
    "    if not a in np.r_[0,1,2,3,7:19,22:df.shape[1]]:\n",
    "        i = df.columns[a]\n",
    "        df[i] = df[i].apply(lambda x : str(x) if pd.isnull(x) else i.replace('精阅体格检查\\n/','') + str(x) )\n",
    "        df[i] = df[i].apply(lambda x : x.replace('伊阅',''))  \n",
    "        df[i] = df[i].apply(lambda x : x.replace('nan', ''))  \n",
    "        #df = df[~df[i].isin([''])]\n",
    "\n",
    "# transform into list and remove float phenotype\n",
    "df = df.T\n",
    "df.columns = df.iloc[0]\n",
    "res = []\n",
    "for i in range(0,df.shape[1]):\n",
    "    res.append(pd.Series(df.iloc[0:,i]).dropna().drop_duplicates().values) \n",
    "for i in range(0,len(res)):\n",
    "    res[i] = [x for x in res[i] if type(x) != float] \n",
    "\n",
    "\n",
    "# tokenization\n",
    "for a in range (0,len(res)):\n",
    "    for b in range(0,len(res[a])):\n",
    "        res[a][b] = re.split('[，]|[,]|[；]|[：]', res[a][b])\n",
    "        \n",
    "# save and export to numpy array\n",
    "#np.save('C://Users/Jingz/OneDrive/CarrierGene/2019.07.10-HPO (en cours)/NLP_for_onedrive/Run/cli_tokenizing_res.npy', np.array(res))\n",
    "\n",
    "# read hpo data\n",
    "hpo = pd.read_csv('C:/Users/Jingz/OneDrive/CarrierGene/2019.07.10-HPO (done)/hpo_import_python/Hpo_cn_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "model = Doc2Vec.load(\"C:/Users/Jingz/OneDrive/CarrierGene/2019.07.10-HPO (done)/NLP_for_onedrive/Not_In_Database/unsup/M_Doc2Vec_BOW.model\")\n",
    "\n",
    "from translate import Translator\n",
    "translator = Translator(from_lang='zh', to_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenization (top 10 most similar docs + top 3 terms search in hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\translate\\translate.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mtext_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRANSLATION_API_MAX_LENGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace_whitespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_translation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_wraped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext_wraped\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\translate\\providers\\mymemory_translated.py\u001b[0m in \u001b[0;36mget_translation\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'matches'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mnext_best_match\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext_best_match\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'translation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-89b5d6c5bb11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pheno_en'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pheno'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pheno_en_tok'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pheno_en'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vec'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pheno_en'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\translate\\translate.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mtext_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRANSLATION_API_MAX_LENGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace_whitespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_translation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_wraped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext_wraped\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "### extract information\n",
    "txt = pd.DataFrame(columns=['id', 'pheno'])\n",
    "for i in range(0,len(res)):\n",
    "    length = len(list(chain(*res[i][2:len(res[0])])) )\n",
    "    tem = pd.DataFrame({'id': pd.DataFrame(res[i][0]).values.repeat(length), 'pheno': list(chain(*res[i][2:len(res[0])]))  })\n",
    "    txt = txt.append(tem)\n",
    "txt['pheno'] = txt['pheno'].dropna()\n",
    "txt = txt.reset_index(drop=True)\n",
    "\n",
    "### clean txt (remove non sense doc)\n",
    "searchfor = [\"正常\"]\n",
    "\n",
    "txt['pheno'] = txt['pheno'].apply(clean_text, lan='cn').dropna()\n",
    "\n",
    "txt = txt[~ txt.pheno.str.contains('|'.join(searchfor), na=False)]\n",
    "txt = txt[~ txt.pheno.isin(['', '.', '重点关注','无','全外扫描','未查','B超','其他遗传学检查结果','其他检查','要求全外扫描'])] # remove str contraing specific str\n",
    "txt['pheno'] = txt['pheno'].apply(lambda x: x.replace('45,X','45_X'))\n",
    "\n",
    "txt = txt.reset_index(drop=True)\n",
    "\n",
    "### find most similar doc using tags\n",
    "txt['pheno_tok'] = txt['pheno_en_tok'] = txt['sim'] = txt['vec'] = txt['pheno_en'] = txt['pheno']\n",
    "\n",
    "for p in range(len(txt)):\n",
    "    txt['pheno_en'][p] = translator.translate(txt['pheno'][p])\n",
    "    txt['pheno_en_tok'][p] = text_to_word_sequence(txt['pheno_en'][p])\n",
    "    txt['vec'][p] = word_tokenize(txt['pheno_en'][p].lower())\n",
    "    txt['vec'][p] = model.infer_vector(txt['vec'][p])\n",
    "    txt['sim'][p] = model.docvecs.most_similar([txt['vec'][p]], topn = topn)\n",
    "\n",
    "txt['pheno_tok'] = txt['pheno'].apply(lambda x: list(jieba.cut(x)))    \n",
    "#txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Translator.translate of <translate.translate.Translator object at 0x000001403946BB50>>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt['doc2vec']  = np.empty((len(txt), 0)).tolist()\n",
    "txt['sent_search']  = np.empty((len(txt), 0)).tolist()\n",
    "txt['word_search']  = np.empty((len(txt), 0)).tolist()\n",
    "txt['jieba_search']  = np.empty((len(txt), 0)).tolist()\n",
    "txt['common_term'] = np.empty((len(txt), 0)).tolist()\n",
    "\n",
    "\n",
    "for a in range(0,len(txt['sim'])):\n",
    "    \n",
    "    \n",
    "# doc2vec search\n",
    "    for b in range(0,len(txt['sim'][a])-1):\n",
    "        index = txt['sim'][a][b][0]\n",
    "        txt['doc2vec'][a].append(str(list(hpo.iloc[:,[0,2]].loc[int(index)]))[1:-1])\n",
    "\n",
    "        \n",
    "# sentence search        \n",
    "    if(len(txt['pheno'][a])>0): # or >1\n",
    "        sent = txt['pheno'][a]  # possible to use n-gram\n",
    "        if (len(hpo[hpo['merge_cn'].str.contains(sent, na=False)])>0):\n",
    "            res = hpo[hpo['merge_cn'].str.contains(sent, na=False)].iloc[:,[0,2]]\n",
    "            if (len(res)<=3): l=len(res)\n",
    "            else: l=3\n",
    "            for c in range(l):\n",
    "                txt['sent_search'][a].append(str(list(res.iloc[c]))[1:-1]) \n",
    "  \n",
    "\n",
    "# word search (english word tokens)      \n",
    "    if(len(text_to_word_sequence(txt['pheno_en'][a]))>0): # or >1\n",
    "        words = text_to_word_sequence(txt['pheno_en'][a])  # possible to use n-gram\n",
    "        for word in words:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            word = re.compile('[0-9]').sub('',word) # remove numbers\n",
    "            word = clean_text(word, lan='en')        # remove stop words                \n",
    "            # word = PorterStemmer().stem(word)\n",
    "            if (len(clean_text(word, lan='en'))>0): \n",
    "                if (len(hpo[hpo['merge'].str.contains(word, na=False)])>0): \n",
    "                    #txt['word_search'][a].append('**'+word +'**')\n",
    "                    res = hpo[hpo['merge'].str.contains(word, na=False)].iloc[:,[0,2]]\n",
    "                    if (len(res)<=3): l=len(res)\n",
    "                    else: l=3\n",
    "                    for c in range(l):\n",
    "                        txt['word_search'][a].append(str(list(res.iloc[c]))[1:-1])\n",
    "    \n",
    "    \n",
    "    # jieba search (chinese tokens)   \n",
    "    if(len(txt['pheno_tok'][a])>0): # or >1\n",
    "        words_z = txt['pheno_tok'][a]  # possible to use n-gram\n",
    "        for word in words_z:\n",
    "            if (len(word)>0):\n",
    "                if (len(hpo[hpo['merge_cn'].str.contains(word, na=False)])>0): \n",
    "                    res = hpo[hpo['merge_cn'].str.contains(word, na=False)].iloc[:,[0,2]]\n",
    "                    if (len(res)<=3): l=len(res)\n",
    "                    else: l=3\n",
    "                    for c in range(l):\n",
    "                        txt['jieba_search'][a].append(str(list(res.iloc[c]))[1:-1]) \n",
    "                        \n",
    "                        \n",
    "\n",
    "# search common terms (no repeat)\n",
    "    txt['common_term'][a] = list(set(list(set(txt['jieba_search'][a])&set(txt['word_search'][a])) + \n",
    "                                     list(set(txt['doc2vec'][a])&set(txt['word_search'][a])) + \n",
    "                                     list(set(txt['jieba_search'][a] )&set(txt['doc2vec'][a])) +\n",
    "                                     list(set(txt['sent_search'][a])&set(txt['doc2vec'][a])) +\n",
    "                                     list(set(txt['sent_search'][a])&set(txt['jieba_search'][a])) + \n",
    "                                     list(set(txt['sent_search'][a])&set(txt['word_search'][a]))                                     \n",
    "                                    ))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "res = txt[['id','pheno','pheno_en','common_term','sent_search','doc2vec', 'jieba_search','word_search']]\n",
    "res.to_csv('C://Users/Jingz/OneDrive/CarrierGene/2019.07.10-HPO (en cours)/NLP_for_onedrive/Run/prediction_unsp_'+ strftime(\"%Y%m%d_%H%M\", localtime()) +'.csv', header=True, index=False, encoding=\"ANSI\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
